# main.py
"""
Problem 6 â€” ì§€í‘œ ê³„ì‚°ê¸° (ìƒì†ê³¼ ì¶”ìƒí™”)
- ML ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” ì¶”ìƒ í´ë˜ìŠ¤ì™€ êµ¬ì²´ í´ë˜ìŠ¤ë“¤ êµ¬í˜„
- ìƒì†, ì¶”ìƒí™”, ë‹¤í˜•ì„± í•™ìŠµ
"""

from abc import ABC, abstractmethod


class Metric(ABC):
    def __init__(self, name: str) -> None:
        """
        ì§€í‘œ ì´ë¦„ì„ ì €ì¥í•˜ëŠ” ê¸°ë³¸ ìƒì„±ì.
        """
        self.name = name

    @abstractmethod
    def compute(self, y_true: list[int], y_pred: list[int]) -> float:
        """
        ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë°›ì•„ ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” ì¶”ìƒ ë©”ì„œë“œ.
        êµ¬ì²´ í´ë˜ìŠ¤ì—ì„œ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        pass

    def evaluate(self, y_true: list[int], y_pred: list[int]) -> str:
        """
        ì§€í‘œë¥¼ ê³„ì‚°í•˜ê³  ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜.
        """
        score = self.compute(y_true, y_pred)
        return f"{self.name}: {score:.3f}"


class Accuracy(Metric):
    def __init__(self) -> None:
        """
        ì •í™•ë„ ì§€í‘œ ì´ˆê¸°í™”.
        """
        super().__init__("Accuracy")

    def compute(self, y_true: list[int], y_pred: list[int]) -> float:
        """
        ì •í™•ë„ ê³„ì‚°: (ë§ì€ ì˜ˆì¸¡ ìˆ˜) / (ì „ì²´ ì˜ˆì¸¡ ìˆ˜)
        """
        if len(y_true) == 0:
            return 0.0

        correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)
        return correct / len(y_true)


class Precision(Metric):
    def __init__(self, positive_class: int = 1) -> None:
        """
        ì •ë°€ë„ ì§€í‘œ ì´ˆê¸°í™”.
        """
        super().__init__("Precision")
        self.positive_class = positive_class

    def compute(self, y_true: list[int], y_pred: list[int]) -> float:
        """
        ì •ë°€ë„ ê³„ì‚°: TP / (TP + FP)
        """
        if len(y_true) == 0:
            return 0.0

        tp = sum(1 for t, p in zip(y_true, y_pred)
                if t == self.positive_class and p == self.positive_class)
        fp = sum(1 for t, p in zip(y_true, y_pred)
                if t != self.positive_class and p == self.positive_class)

        if tp + fp == 0:
            return 0.0

        return tp / (tp + fp)


class Recall(Metric):
    def __init__(self, positive_class: int = 1) -> None:
        """
        ì¬í˜„ìœ¨ ì§€í‘œ ì´ˆê¸°í™”.
        """
        super().__init__("Recall")
        self.positive_class = positive_class

    def compute(self, y_true: list[int], y_pred: list[int]) -> float:
        """
        ì¬í˜„ìœ¨ ê³„ì‚°: TP / (TP + FN)
        """
        if len(y_true) == 0:
            return 0.0

        tp = sum(1 for t, p in zip(y_true, y_pred)
                if t == self.positive_class and p == self.positive_class)
        fn = sum(1 for t, p in zip(y_true, y_pred)
                if t == self.positive_class and p != self.positive_class)

        if tp + fn == 0:
            return 0.0

        return tp / (tp + fn)


if __name__ == "__main__":
    # Demo runs only when executed directly
    def run_demo():
        print("ğŸ“Š ML Metrics Calculator Demo")
        print("=" * 40)

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        y_true = [1, 0, 1, 1, 0, 1, 0, 0]
        y_pred = [1, 0, 0, 1, 0, 1, 1, 0]

        print(f"\nğŸ“‹ Test Data:")
        print(f"y_true: {y_true}")
        print(f"y_pred: {y_pred}")

        # ì§€í‘œ ê°ì²´ ìƒì„±
        accuracy = Accuracy()
        precision = Precision(positive_class=1)
        recall = Recall(positive_class=1)

        print(f"\nğŸ” Inheritance Check:")
        print(f"accuracy isinstance Metric: {isinstance(accuracy, Metric)}")
        print(f"precision isinstance Metric: {isinstance(precision, Metric)}")
        print(f"recall isinstance Metric: {isinstance(recall, Metric)}")

        print(f"\nğŸ“ˆ Individual Metrics:")
        print(f"Accuracy: {accuracy.compute(y_true, y_pred):.3f}")
        print(f"Precision: {precision.compute(y_true, y_pred):.3f}")
        print(f"Recall: {recall.compute(y_true, y_pred):.3f}")

        print(f"\nğŸ“ Formatted Results:")
        print(accuracy.evaluate(y_true, y_pred))
        print(precision.evaluate(y_true, y_pred))
        print(recall.evaluate(y_true, y_pred))

        print(f"\nğŸ¯ Polymorphism Demo:")
        metrics = [accuracy, precision, recall]
        for metric in metrics:
            result = metric.evaluate(y_true, y_pred)
            print(f"  {result}")

        print(f"\nğŸ§® Detailed Calculation Example:")
        # ìƒì„¸ ê³„ì‚° ì˜ˆì‹œ
        y_example = [1, 0, 1, 1]
        p_example = [1, 0, 0, 1]

        print(f"Example: y_true={y_example}, y_pred={p_example}")

        # ìˆ˜ë™ ê³„ì‚°
        correct = sum(1 for t, p in zip(y_example, p_example) if t == p)
        acc = correct / len(y_example)
        print(f"  Accuracy: {correct}/{len(y_example)} = {acc:.3f}")

        tp = sum(1 for t, p in zip(y_example, p_example) if t == 1 and p == 1)
        fp = sum(1 for t, p in zip(y_example, p_example) if t == 0 and p == 1)
        fn = sum(1 for t, p in zip(y_example, p_example) if t == 1 and p == 0)

        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0

        print(f"  TP={tp}, FP={fp}, FN={fn}")
        print(f"  Precision: {tp}/({tp}+{fp}) = {prec:.3f}")
        print(f"  Recall: {tp}/({tp}+{fn}) = {rec:.3f}")

        # ì¶”ìƒ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸
        print(f"\nâš ï¸ Abstract Class Test:")
        try:
            metric = Metric("Test")
            print("ERROR: Should not be able to instantiate abstract class!")
        except TypeError as e:
            print(f"âœ… Correctly prevented: {e}")

    run_demo()